{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f4fe192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "import spacy\n",
    "# spacy.cli.download(\"nl_core_news_lg\")\n",
    "nlp = spacy.load(\"nl_core_news_lg\") \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import textdescriptives as td\n",
    "\n",
    "import folia.main as folia #https://foliapy.readthedocs.io/en/latest/folia.html#loading-a-document\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "BSdir = Path('/Users/sabijn/Documents/PhD/Datasets/BS_only_narrative')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3691b700",
   "metadata": {},
   "source": [
    "## Create Basiscript lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fc0436",
   "metadata": {},
   "outputs": [],
   "source": [
    "BSfile = Path(os.getcwd()) / Path('datasets/BSlexicon.csv')\n",
    "\n",
    "if BSfile.exists():\n",
    "    df_lexicon = pd.read_csv(BSfile)\n",
    "else:\n",
    "    BSlexicon = defaultdict(int)\n",
    "    for file in tqdm(BSdir.glob('*.xml')):\n",
    "        doc = folia.Document(file=str(file))\n",
    "        enriched_doc = nlp(doc.text())\n",
    "        for token in enriched_doc:\n",
    "            BSlexicon[(token.lemma_, token.pos_)] += 1\n",
    "\n",
    "    df_lexicon = pd.DataFrame([(k[0], k[1], v) for k, v in BSlexicon.items()],\n",
    "        columns=['lemma', 'pos', 'freq'])\n",
    "\n",
    "    df_lexicon.to_csv('datasets/BSlexicon.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340bfea9",
   "metadata": {},
   "source": [
    "## Create Basiscript bigram lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "085fd46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d879724a283b47daa880d1ea8cd77c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BSfile = Path('/Users/sabijn/Documents/PhD/code/storylm_p1_data/datasets/BS_bigram_lexicon_part.csv')\n",
    "\n",
    "if BSfile.exists():\n",
    "    df_bi_lexicon = pd.read_csv(BSfile)\n",
    "else:\n",
    "    BS_bi_lexicon = defaultdict(int)\n",
    "    for file in tqdm(BSdir.glob('*.xml')):\n",
    "        doc = folia.Document(file=str(file))\n",
    "        enriched_doc = nlp(doc.text())\n",
    "\n",
    "        for i in range(len(enriched_doc) - 1):\n",
    "            BS_bi_lexicon[((enriched_doc[i].lemma_, enriched_doc[i + 1].lemma_), (enriched_doc[i].pos_, enriched_doc[i + 1].pos_))] += 1\n",
    "        \n",
    "        if i == 100_000:\n",
    "            break\n",
    "\n",
    "    df_bi_lexicon = pd.DataFrame([(k[0], k[1], v) for k, v in BS_bi_lexicon.items()],\n",
    "        columns=['lemma', 'pos', 'freq'])\n",
    "\n",
    "    df_bi_lexicon.to_csv(BSfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2907e83f",
   "metadata": {},
   "source": [
    "## Create dependency-constrained lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7e79b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_dep_pairs(node, selected):\n",
    "    temp = {child.dep_ : child for child in list(node.children)}\n",
    "\n",
    "    if node.dep_ in ['ROOT', 'ccomp', 'xcomp', 'conj']:\n",
    "        for (child_dep, child) in temp.items():\n",
    "            if child_dep in ['nsubj', 'obj', 'cop', 'nsubj:pass'] and child.pos_ != 'PRON' and child.text != 'einde':\n",
    "                selected[((child.lemma_, node.lemma_), (child_dep, node.dep_))] += 1\n",
    "    \n",
    "    if node.pos_ == 'NOUN':\n",
    "        for child in list(node.children):\n",
    "            if child.pos_ == 'ADJ' and child.dep_ == 'amod':\n",
    "                selected[((child.lemma_, node.lemma_), (child.dep_, node.dep_))] += 1\n",
    "\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        for child in node.children:\n",
    "            select_dep_pairs(child, selected=selected)\n",
    "\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f35c0c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '/Users/sabijn/Documents/PhD/code/storylm_p1_data/notebooks/datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     16\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     18\u001b[39m df_dp_lexicon = pd.DataFrame([(k[\u001b[32m0\u001b[39m], k[\u001b[32m1\u001b[39m], v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m BS_dep_lexicon.items()],\n\u001b[32m     19\u001b[39m     columns=[\u001b[33m'\u001b[39m\u001b[33mlemma\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpos\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mfreq\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m df_dp_lexicon.to_csv(BSfile)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/storylmdata/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/storylmdata/lib/python3.12/site-packages/pandas/core/generic.py:3967\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3956\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3958\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3959\u001b[39m     frame=df,\n\u001b[32m   3960\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3964\u001b[39m     decimal=decimal,\n\u001b[32m   3965\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3967\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter).to_csv(\n\u001b[32m   3968\u001b[39m     path_or_buf,\n\u001b[32m   3969\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   3970\u001b[39m     sep=sep,\n\u001b[32m   3971\u001b[39m     encoding=encoding,\n\u001b[32m   3972\u001b[39m     errors=errors,\n\u001b[32m   3973\u001b[39m     compression=compression,\n\u001b[32m   3974\u001b[39m     quoting=quoting,\n\u001b[32m   3975\u001b[39m     columns=columns,\n\u001b[32m   3976\u001b[39m     index_label=index_label,\n\u001b[32m   3977\u001b[39m     mode=mode,\n\u001b[32m   3978\u001b[39m     chunksize=chunksize,\n\u001b[32m   3979\u001b[39m     quotechar=quotechar,\n\u001b[32m   3980\u001b[39m     date_format=date_format,\n\u001b[32m   3981\u001b[39m     doublequote=doublequote,\n\u001b[32m   3982\u001b[39m     escapechar=escapechar,\n\u001b[32m   3983\u001b[39m     storage_options=storage_options,\n\u001b[32m   3984\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/storylmdata/lib/python3.12/site-packages/pandas/io/formats/format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m csv_formatter.save()\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/storylmdata/lib/python3.12/site-packages/pandas/io/formats/csvs.py:251\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03mCreate the writer & save.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[32m    252\u001b[39m     \u001b[38;5;28mself\u001b[39m.filepath_or_buffer,\n\u001b[32m    253\u001b[39m     \u001b[38;5;28mself\u001b[39m.mode,\n\u001b[32m    254\u001b[39m     encoding=\u001b[38;5;28mself\u001b[39m.encoding,\n\u001b[32m    255\u001b[39m     errors=\u001b[38;5;28mself\u001b[39m.errors,\n\u001b[32m    256\u001b[39m     compression=\u001b[38;5;28mself\u001b[39m.compression,\n\u001b[32m    257\u001b[39m     storage_options=\u001b[38;5;28mself\u001b[39m.storage_options,\n\u001b[32m    258\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m._save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/storylmdata/lib/python3.12/site-packages/pandas/io/common.py:749\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m     check_parent_directory(\u001b[38;5;28mstr\u001b[39m(handle))\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression != \u001b[33m\"\u001b[39m\u001b[33mzstd\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/storylmdata/lib/python3.12/site-packages/pandas/io/common.py:616\u001b[39m, in \u001b[36mcheck_parent_directory\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    614\u001b[39m parent = Path(path).parent\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent.is_dir():\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot save file into a non-existent directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Cannot save file into a non-existent directory: '/Users/sabijn/Documents/PhD/code/storylm_p1_data/notebooks/datasets'"
     ]
    }
   ],
   "source": [
    "BSfile = Path('/Users/sabijn/Documents/PhD/code/storylm_p1_data/datasets/BSdeplexicon_part.csv')\n",
    "\n",
    "\n",
    "if BSfile.exists():\n",
    "    df_dep_lexicon = pd.read_csv(BSfile)\n",
    "else:\n",
    "    BS_dep_lexicon = defaultdict(int)\n",
    "    for i, file in enumerate(BSdir.glob('*.xml')):\n",
    "        doc = nlp(folia.Document(file=str(file)).text())\n",
    "\n",
    "        for sent in doc.sents:\n",
    "            pair_dict = select_dep_pairs(sent.root, BS_dep_lexicon)\n",
    "\n",
    "        if i == 100_000:\n",
    "            break\n",
    "\n",
    "    df_dp_lexicon = pd.DataFrame([(k[0], k[1], v) for k, v in BS_dep_lexicon.items()],\n",
    "        columns=['lemma', 'pos', 'freq'])\n",
    "\n",
    "    df_dp_lexicon.to_csv(BSfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4463ca0b",
   "metadata": {},
   "source": [
    "## Transform dep to no dep lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b569088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast \n",
    "\n",
    "new_lexicon_file = Path('/Users/sabijn/Documents/PhD/code/storylm_p1_data/datasets/BSdep_without_deps_part.csv')\n",
    "old_lexicon_file = Path('/Users/sabijn/Documents/PhD/code/storylm_p1_data/datasets/BSdeplexicon_part.csv')\n",
    "df_old = pd.read_csv(old_lexicon_file)\n",
    "\n",
    "df_stripped = df_old.drop(columns='pos')\n",
    "df_grouped = df_old.groupby('lemma', as_index=False)['freq'].sum()\n",
    "\n",
    "# Convert string to tuple\n",
    "df_grouped['lemma'] = df_grouped['lemma'].apply(ast.literal_eval)\n",
    "\n",
    "# Split the tuple into two separate columns\n",
    "df_grouped[['lemma1', 'lemma2']] = pd.DataFrame(df_grouped['lemma'].tolist(), index=df_grouped.index)\n",
    "\n",
    "# Drop the original tuple column\n",
    "df_grouped = df_grouped.drop(columns='lemma')\n",
    "\n",
    "df_grouped.to_csv(new_lexicon_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a8bdd80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 142935,\n",
       "         2: 23410,\n",
       "         3: 9001,\n",
       "         4: 5094,\n",
       "         5: 3075,\n",
       "         6: 2140,\n",
       "         7: 1568,\n",
       "         8: 1212,\n",
       "         9: 908,\n",
       "         10: 800,\n",
       "         11: 641,\n",
       "         12: 560,\n",
       "         13: 460,\n",
       "         14: 427,\n",
       "         16: 320,\n",
       "         15: 319,\n",
       "         18: 274,\n",
       "         17: 262,\n",
       "         20: 228,\n",
       "         19: 216,\n",
       "         22: 181,\n",
       "         21: 178,\n",
       "         23: 173,\n",
       "         24: 158,\n",
       "         25: 141,\n",
       "         26: 132,\n",
       "         27: 116,\n",
       "         29: 112,\n",
       "         32: 96,\n",
       "         30: 95,\n",
       "         31: 91,\n",
       "         34: 90,\n",
       "         28: 86,\n",
       "         33: 74,\n",
       "         36: 69,\n",
       "         35: 68,\n",
       "         37: 63,\n",
       "         38: 58,\n",
       "         42: 57,\n",
       "         40: 54,\n",
       "         46: 46,\n",
       "         44: 46,\n",
       "         39: 46,\n",
       "         41: 45,\n",
       "         43: 44,\n",
       "         47: 44,\n",
       "         52: 38,\n",
       "         56: 37,\n",
       "         50: 36,\n",
       "         57: 35,\n",
       "         53: 34,\n",
       "         49: 34,\n",
       "         48: 33,\n",
       "         59: 32,\n",
       "         45: 31,\n",
       "         51: 31,\n",
       "         54: 30,\n",
       "         64: 27,\n",
       "         55: 26,\n",
       "         62: 23,\n",
       "         60: 23,\n",
       "         67: 23,\n",
       "         69: 23,\n",
       "         66: 23,\n",
       "         68: 20,\n",
       "         63: 20,\n",
       "         65: 20,\n",
       "         84: 19,\n",
       "         71: 18,\n",
       "         73: 18,\n",
       "         72: 18,\n",
       "         58: 17,\n",
       "         95: 17,\n",
       "         80: 17,\n",
       "         81: 16,\n",
       "         61: 16,\n",
       "         92: 15,\n",
       "         79: 15,\n",
       "         75: 15,\n",
       "         77: 14,\n",
       "         74: 14,\n",
       "         78: 13,\n",
       "         112: 13,\n",
       "         116: 12,\n",
       "         70: 12,\n",
       "         85: 12,\n",
       "         113: 12,\n",
       "         86: 12,\n",
       "         76: 11,\n",
       "         100: 11,\n",
       "         97: 11,\n",
       "         91: 10,\n",
       "         93: 10,\n",
       "         87: 10,\n",
       "         88: 10,\n",
       "         114: 9,\n",
       "         90: 9,\n",
       "         82: 9,\n",
       "         98: 9,\n",
       "         109: 9,\n",
       "         120: 9,\n",
       "         124: 9,\n",
       "         89: 9,\n",
       "         94: 9,\n",
       "         142: 9,\n",
       "         150: 8,\n",
       "         106: 8,\n",
       "         99: 8,\n",
       "         83: 8,\n",
       "         136: 8,\n",
       "         144: 7,\n",
       "         107: 7,\n",
       "         122: 7,\n",
       "         159: 7,\n",
       "         186: 6,\n",
       "         105: 6,\n",
       "         158: 6,\n",
       "         117: 6,\n",
       "         176: 6,\n",
       "         127: 6,\n",
       "         110: 6,\n",
       "         101: 6,\n",
       "         137: 6,\n",
       "         119: 6,\n",
       "         102: 6,\n",
       "         125: 5,\n",
       "         153: 5,\n",
       "         347: 5,\n",
       "         143: 5,\n",
       "         139: 5,\n",
       "         126: 5,\n",
       "         217: 5,\n",
       "         163: 5,\n",
       "         131: 5,\n",
       "         121: 4,\n",
       "         161: 4,\n",
       "         130: 4,\n",
       "         162: 4,\n",
       "         157: 4,\n",
       "         220: 4,\n",
       "         104: 4,\n",
       "         115: 4,\n",
       "         178: 4,\n",
       "         129: 4,\n",
       "         221: 4,\n",
       "         239: 4,\n",
       "         135: 4,\n",
       "         207: 4,\n",
       "         154: 4,\n",
       "         182: 4,\n",
       "         194: 4,\n",
       "         103: 4,\n",
       "         236: 4,\n",
       "         152: 4,\n",
       "         190: 3,\n",
       "         185: 3,\n",
       "         160: 3,\n",
       "         258: 3,\n",
       "         111: 3,\n",
       "         96: 3,\n",
       "         188: 3,\n",
       "         187: 3,\n",
       "         266: 3,\n",
       "         180: 3,\n",
       "         333: 3,\n",
       "         134: 3,\n",
       "         181: 3,\n",
       "         172: 3,\n",
       "         173: 3,\n",
       "         242: 3,\n",
       "         133: 3,\n",
       "         200: 3,\n",
       "         358: 3,\n",
       "         166: 3,\n",
       "         280: 3,\n",
       "         201: 3,\n",
       "         235: 3,\n",
       "         243: 3,\n",
       "         192: 3,\n",
       "         128: 3,\n",
       "         193: 3,\n",
       "         164: 3,\n",
       "         252: 3,\n",
       "         108: 3,\n",
       "         231: 3,\n",
       "         170: 3,\n",
       "         118: 3,\n",
       "         215: 3,\n",
       "         232: 2,\n",
       "         360: 2,\n",
       "         254: 2,\n",
       "         208: 2,\n",
       "         245: 2,\n",
       "         169: 2,\n",
       "         415: 2,\n",
       "         287: 2,\n",
       "         196: 2,\n",
       "         248: 2,\n",
       "         318: 2,\n",
       "         249: 2,\n",
       "         228: 2,\n",
       "         205: 2,\n",
       "         202: 2,\n",
       "         437: 2,\n",
       "         1138: 2,\n",
       "         234: 2,\n",
       "         306: 2,\n",
       "         377: 2,\n",
       "         557: 2,\n",
       "         515: 2,\n",
       "         260: 2,\n",
       "         331: 2,\n",
       "         353: 2,\n",
       "         261: 2,\n",
       "         616: 2,\n",
       "         198: 2,\n",
       "         315: 2,\n",
       "         541: 2,\n",
       "         376: 2,\n",
       "         216: 2,\n",
       "         140: 2,\n",
       "         145: 2,\n",
       "         214: 2,\n",
       "         462: 2,\n",
       "         199: 2,\n",
       "         265: 2,\n",
       "         167: 2,\n",
       "         223: 2,\n",
       "         183: 2,\n",
       "         434: 2,\n",
       "         184: 2,\n",
       "         495: 2,\n",
       "         612: 2,\n",
       "         213: 2,\n",
       "         165: 2,\n",
       "         206: 2,\n",
       "         319: 2,\n",
       "         146: 2,\n",
       "         229: 2,\n",
       "         218: 2,\n",
       "         189: 2,\n",
       "         209: 2,\n",
       "         168: 2,\n",
       "         212: 2,\n",
       "         316: 2,\n",
       "         636: 2,\n",
       "         241: 2,\n",
       "         195: 2,\n",
       "         155: 2,\n",
       "         179: 2,\n",
       "         516: 2,\n",
       "         406: 2,\n",
       "         251: 2,\n",
       "         267: 2,\n",
       "         233: 2,\n",
       "         339: 2,\n",
       "         141: 2,\n",
       "         524: 1,\n",
       "         296: 1,\n",
       "         812: 1,\n",
       "         754: 1,\n",
       "         980: 1,\n",
       "         455: 1,\n",
       "         1236: 1,\n",
       "         799: 1,\n",
       "         582: 1,\n",
       "         665: 1,\n",
       "         132: 1,\n",
       "         156: 1,\n",
       "         382: 1,\n",
       "         238: 1,\n",
       "         546: 1,\n",
       "         1288: 1,\n",
       "         487: 1,\n",
       "         570: 1,\n",
       "         138: 1,\n",
       "         1675: 1,\n",
       "         581: 1,\n",
       "         441: 1,\n",
       "         617: 1,\n",
       "         1963: 1,\n",
       "         247: 1,\n",
       "         675: 1,\n",
       "         273: 1,\n",
       "         313: 1,\n",
       "         420: 1,\n",
       "         256: 1,\n",
       "         385: 1,\n",
       "         341: 1,\n",
       "         948: 1,\n",
       "         448: 1,\n",
       "         304: 1,\n",
       "         359: 1,\n",
       "         583: 1,\n",
       "         451: 1,\n",
       "         509: 1,\n",
       "         259: 1,\n",
       "         538: 1,\n",
       "         1698: 1,\n",
       "         1445: 1,\n",
       "         149: 1,\n",
       "         404: 1,\n",
       "         363: 1,\n",
       "         930: 1,\n",
       "         532: 1,\n",
       "         436: 1,\n",
       "         374: 1,\n",
       "         3356: 1,\n",
       "         608: 1,\n",
       "         3797: 1,\n",
       "         729: 1,\n",
       "         203: 1,\n",
       "         191: 1,\n",
       "         433: 1,\n",
       "         727: 1,\n",
       "         1479: 1,\n",
       "         1067: 1,\n",
       "         1243: 1,\n",
       "         626: 1,\n",
       "         521: 1,\n",
       "         491: 1,\n",
       "         576: 1,\n",
       "         250: 1,\n",
       "         671: 1,\n",
       "         452: 1,\n",
       "         494: 1,\n",
       "         564: 1,\n",
       "         475: 1,\n",
       "         408: 1,\n",
       "         1307: 1,\n",
       "         123: 1,\n",
       "         402: 1,\n",
       "         1204: 1,\n",
       "         445: 1,\n",
       "         562: 1,\n",
       "         423: 1,\n",
       "         275: 1,\n",
       "         432: 1,\n",
       "         340: 1,\n",
       "         307: 1,\n",
       "         411: 1,\n",
       "         825: 1,\n",
       "         468: 1,\n",
       "         1131: 1,\n",
       "         338: 1,\n",
       "         695: 1,\n",
       "         828: 1,\n",
       "         270: 1,\n",
       "         343: 1,\n",
       "         943: 1,\n",
       "         539: 1,\n",
       "         685: 1,\n",
       "         197: 1,\n",
       "         283: 1,\n",
       "         478: 1,\n",
       "         210: 1,\n",
       "         657: 1,\n",
       "         297: 1,\n",
       "         317: 1,\n",
       "         369: 1,\n",
       "         224: 1,\n",
       "         399: 1,\n",
       "         472: 1,\n",
       "         511: 1,\n",
       "         393: 1,\n",
       "         379: 1,\n",
       "         979: 1,\n",
       "         658: 1,\n",
       "         298: 1,\n",
       "         334: 1,\n",
       "         349: 1,\n",
       "         293: 1,\n",
       "         944: 1,\n",
       "         290: 1,\n",
       "         222: 1,\n",
       "         403: 1,\n",
       "         346: 1,\n",
       "         321: 1,\n",
       "         2826: 1,\n",
       "         832: 1,\n",
       "         807: 1,\n",
       "         1080: 1,\n",
       "         151: 1,\n",
       "         2145: 1,\n",
       "         279: 1,\n",
       "         383: 1,\n",
       "         308: 1,\n",
       "         460: 1,\n",
       "         587: 1,\n",
       "         378: 1,\n",
       "         710: 1,\n",
       "         357: 1,\n",
       "         417: 1,\n",
       "         439: 1,\n",
       "         253: 1,\n",
       "         227: 1,\n",
       "         595: 1,\n",
       "         1677: 1,\n",
       "         1401: 1,\n",
       "         666: 1,\n",
       "         2618: 1,\n",
       "         682: 1,\n",
       "         328: 1,\n",
       "         790: 1,\n",
       "         1102: 1,\n",
       "         694: 1,\n",
       "         469: 1,\n",
       "         438: 1,\n",
       "         3215: 1,\n",
       "         640: 1,\n",
       "         352: 1,\n",
       "         717: 1,\n",
       "         1171: 1,\n",
       "         2230: 1,\n",
       "         5161: 1,\n",
       "         925: 1,\n",
       "         2146: 1,\n",
       "         1782: 1,\n",
       "         633: 1,\n",
       "         329: 1,\n",
       "         354: 1,\n",
       "         691: 1,\n",
       "         940: 1,\n",
       "         449: 1,\n",
       "         906: 1,\n",
       "         1126: 1,\n",
       "         305: 1,\n",
       "         610: 1,\n",
       "         1666: 1,\n",
       "         1535: 1,\n",
       "         611: 1,\n",
       "         627: 1,\n",
       "         342: 1,\n",
       "         389: 1,\n",
       "         935: 1,\n",
       "         16786: 1,\n",
       "         2996: 1,\n",
       "         175: 1,\n",
       "         618: 1,\n",
       "         821: 1,\n",
       "         647: 1,\n",
       "         1096: 1,\n",
       "         476: 1,\n",
       "         327: 1,\n",
       "         819: 1,\n",
       "         372: 1,\n",
       "         219: 1,\n",
       "         770: 1,\n",
       "         1093: 1,\n",
       "         993: 1,\n",
       "         386: 1,\n",
       "         705: 1,\n",
       "         629: 1,\n",
       "         337: 1,\n",
       "         335: 1,\n",
       "         276: 1,\n",
       "         573: 1,\n",
       "         413: 1,\n",
       "         299: 1,\n",
       "         1125: 1,\n",
       "         1001: 1,\n",
       "         773: 1,\n",
       "         545: 1,\n",
       "         699: 1,\n",
       "         1271: 1,\n",
       "         336: 1,\n",
       "         972: 1,\n",
       "         373: 1,\n",
       "         226: 1,\n",
       "         765: 1,\n",
       "         518: 1,\n",
       "         580: 1,\n",
       "         2330: 1,\n",
       "         945: 1,\n",
       "         272: 1,\n",
       "         320: 1,\n",
       "         1430: 1})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "Counter(df_grouped['freq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc55c74f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c1fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BSfile_unigram = Path('/Users/sabijn/Documents/PhD/code/storylm_p1_data/datasets/BS_unigrams_part.csv')\n",
    "\n",
    "if BSfile_unigram.exists():\n",
    "    df_lexicon = pd.read_csv(BSfile_unigram)\n",
    "else:\n",
    "    BSlexicon = defaultdict(int)\n",
    "    for file in tqdm(BSdir.glob('*.xml')):\n",
    "        doc = folia.Document(file=str(file))\n",
    "        enriched_doc = nlp(doc.text())\n",
    "        for token in enriched_doc:\n",
    "            BSlexicon[token.pos_] += 1\n",
    "\n",
    "        if i == 100_000:\n",
    "            break\n",
    "\n",
    "    df_lexicon = pd.DataFrame([(k[0], k[1], v) for k, v in BSlexicon.items()],\n",
    "        columns=['pos', 'freq'])\n",
    "\n",
    "    df_lexicon.to_csv(BSfile_unigram)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "storylmdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
